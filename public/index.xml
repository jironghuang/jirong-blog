<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jirong&#39;s sandbox on Jirong&#39;s sandbox</title>
    <link>/</link>
    <description>Recent content in Jirong&#39;s sandbox on Jirong&#39;s sandbox</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Building a decision tree algorithm from scratch</title>
      <link>/post/building-adecision-tree-from-scratch/</link>
      <pubDate>Fri, 15 Feb 2019 13:09:44 +0800</pubDate>
      
      <guid>/post/building-adecision-tree-from-scratch/</guid>
      <description>

&lt;h2 id=&#34;building-a-decision-tree-from-scratch&#34;&gt;Building a decision tree from scratch&lt;/h2&gt;

&lt;p&gt;Sometimes to truly understand and internalise an algorithm, it&amp;rsquo;s always useful to build from scratch. Rather than relying on a module or library written by someone else.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m fortunate to be given the chance to do it in 1 of my assignments for decision trees.&lt;/p&gt;

&lt;p&gt;From this exercise, I had to rely on my knowledge on recursion, binary trees (in-order traversal) and object oriented programming.&lt;/p&gt;

&lt;p&gt;Below is a snippet of method in a class. The algorithm works as follows,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First you define a leaf size i.e. the maximum number of data you allow to be left in the leaf node.&lt;/li&gt;
&lt;li&gt;If number of data in leaf node is still more than the allowable size, check if all data is the same. If it&amp;rsquo;s the same, return just 1 value&lt;/li&gt;
&lt;li&gt;Next I find the feature based on best correlation (gini coefficient and information gain works too) with the dependent variable values. Note that as you traverse down the tree, this dataset gets smaller&lt;/li&gt;
&lt;li&gt;With the best feature found in each split, I proceed to contruct the left tree and right tree together with a root node&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The idea is that if you are left with a node that&amp;rsquo;s smaller or equals to allowable leaf size, it will stop traversing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/Building_decision_tree.png&#34; alt=&#34;/post/img/Building_decision_tree.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Martingale Strategy - Double Down</title>
      <link>/post/martingale-strategy/</link>
      <pubDate>Sat, 26 Jan 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/martingale-strategy/</guid>
      <description>

&lt;h2 id=&#34;martingale-strategy&#34;&gt;Martingale Strategy&lt;/h2&gt;

&lt;p&gt;In this post, I will simulate a martingale strategy in Roulette&amp;rsquo;s context to highlight the potential risks associated with this strategy.&lt;/p&gt;

&lt;p&gt;Double down! That&amp;rsquo;s essentially the essence of it.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a simple explanation of the strategy,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The croupier spins the ball. If it&amp;rsquo;s red you win the amount you bet, black you lose the same amount&lt;/li&gt;
&lt;li&gt;If you win, you continue to bet the same amount (same as your 1st bet amount)&lt;/li&gt;
&lt;li&gt;If you lose, you double your bet amount&lt;/li&gt;
&lt;li&gt;And if your accumulated winnings hits a certain amount, you stop and leave the casino&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So how would the strategy fare? To explain, I will use Monte Carlo with a Bernoulli distribution for each roulette spin (X ~ B(1, 0.48)).&lt;/p&gt;

&lt;h3 id=&#34;simulate-strategy-10-runs-times&#34;&gt;Simulate strategy 10 runs/ times&lt;/h3&gt;

&lt;p&gt;Here, I simulate the strategy of 10 times. Think of it in this way, there&amp;rsquo;re 10 alternate universes which you exist and you play the same game 10 times. Or you can just simply treat this as going back to the casino on 10 separate days.&lt;/p&gt;

&lt;p&gt;In the chart below, you will notice that for some runs; because of a sequence of bad luck, the losses quick spiralled!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/10_simulations.png&#34; alt=&#34;/post/img/10_simulations.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;simulate-strategy-1000-runs-times-and-compute-the-mean&#34;&gt;Simulate strategy 1000 runs/ times and compute the mean&lt;/h3&gt;

&lt;p&gt;Next, for a more robust interpretation, I went on to simulate this strategy 1000 times and computed the mean and standard deviation. You will notice that the strategy eventually converges to a desired terminal value. In this case, it&amp;rsquo;s $80. So essentially, out of 1000 simulations, all of them reaches my profit target!&lt;/p&gt;

&lt;p&gt;However the the risk is enormous. Near the average 120th run, the standard deviation sky-rocketed to 120,000. I&amp;rsquo;m unsure if anyone could stomach this loss at any one point of time. The journey matters!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/1000_simulations.png&#34; alt=&#34;/post/img/1000_simulations.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/simulation_sd.png&#34; alt=&#34;/post/img/simulation_sd.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;insights-from-this-strategy&#34;&gt;Insights from this strategy&lt;/h3&gt;

&lt;p&gt;Martingale strategy - to put it simply - is a win small, lose potentially huge strategy.&lt;/p&gt;

&lt;p&gt;In this strategy, you will win 100% of the time.&lt;/p&gt;

&lt;p&gt;But the question is, do you have the money (or infinite bankroll) to tide through tough times?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Create a Python Environment in Ubuntu or any Debian-based system</title>
      <link>/post/how-to-create-a-python-environment-in-ubuntu/</link>
      <pubDate>Wed, 09 Jan 2019 23:39:22 +0800</pubDate>
      
      <guid>/post/how-to-create-a-python-environment-in-ubuntu/</guid>
      <description>&lt;p&gt;Often, certain projects or classes involving python require a set of modules/packages for the code to work.&lt;/p&gt;

&lt;p&gt;1 solution is to create a Python Environment dedicated to that project.&lt;/p&gt;

&lt;p&gt;First set up a folder, and include a .yml file with the specific modules and environment that you wish to install. Here is an example (env.yml),&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name: env
channels: !!python/tuple
- !!python/unicode &#39;defaults&#39;
dependencies:
- nb_conda=2.2.0=py27_0
- python=2.7.13=0
- cycler=0.10.0
- functools32=3.2.3.2
- matplotlib=2.0.2
- numpy=1.13.1
- pandas=0.20.3
- py=1.4.34
- pyparsing=2.2.0
- pytest=3.2.1
- python-dateutil=2.6.1
- pytz=2017.2
- scipy=0.19.1
- six=1.10.0
- subprocess32=3.2.7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have anaconda installed, navigate to the folder with .yml; right click and select open in terminal. Then, type the following into bash&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda env create -f env.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once installed, type the following into bash to bring up the environment,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;source activate env
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you wish to install a specific program in this environment - say spyder - you can install it directly. Example,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda install spyder

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To open spyder program, simply type spyder into terminal. And there you go!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Translating Ernest Chan Kalman Filter Strategy Matlab and Python Code Into R</title>
      <link>/post/translating-ernest-chan-kalman-filter-strategy-matlab-and-python-code-into-r/</link>
      <pubDate>Tue, 01 Jan 2019 00:15:53 +0800</pubDate>
      
      <guid>/post/translating-ernest-chan-kalman-filter-strategy-matlab-and-python-code-into-r/</guid>
      <description>

&lt;h2 id=&#34;translating-ernest-chan-kalman-filter-strategy-matlab-and-python-code-into-r&#34;&gt;Translating Ernest Chan Kalman Filter Strategy Matlab and Python Code Into R&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m really intrigued by Ernest Chan&amp;rsquo;s approach in Quant Trading.&lt;/p&gt;

&lt;p&gt;Often in the retail trading space, what &amp;lsquo;gurus&amp;rsquo; preach often sounds really dubious. But Ernest Chan is different. He&amp;rsquo;s sincere, down-to-earth and earnest (meant to be a pun here).&lt;/p&gt;

&lt;p&gt;In my first month of deploying algo trading strategies, I focus mainly on mean-reversion strategies - paricularly amongst pairs. What I learnt - with real capital - is that the hedge ratio is dynamic and will vary over time. In the early days, I fixed it through linear regression. But boy this doesn&amp;rsquo;t work! It&amp;rsquo;s not really market neutral because of the imbalance in values between pairs across time.&lt;/p&gt;

&lt;p&gt;Then I chanced upon Kalman filter - something I learnt during my AI module in my Computer Science Degree days. I&amp;rsquo;ll spare the Math here. It&amp;rsquo;s a variant of the markov model, that uses a series of measurements over time (in this case, one of the pairs price), containing noise and produces estimates of unknown (here it&amp;rsquo;s the hedge ratio and intercept). Hedge ratio is updated in each time step.&lt;/p&gt;

&lt;p&gt;I saw the Python code online for EWA-EWC pair strategy that returns a sharpe ratio of 2.4. I tried to search for a R version but to no avail!&lt;/p&gt;

&lt;p&gt;Hence I decided to spend a day translating the python code into R code (for deployment purposes. Currently my algo trading stack is built around R). Thankfully I&amp;rsquo;m not translating the Matlab version because I do not have prior experience in that. And it would definitely take me more than a day for the translation.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve since,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Wrapped the code with a function&lt;/li&gt;
&lt;li&gt;Loop through a 40 choose 2 combinations of country pairs&lt;/li&gt;
&lt;li&gt;Triangulated with distance metrics like Correlation, Euclidean Distance and Manhattan Distance&lt;/li&gt;
&lt;li&gt;Filtered out long half life (i.e. # of days before reverting to the mean)&lt;/li&gt;
&lt;li&gt;Filtered by sharpe ratios, drawdown and average profits&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And so far, this market-neutral strategy is really promising!&lt;/p&gt;

&lt;h3 id=&#34;translated-r-code-for-ewa-ewc-strategy&#34;&gt;Translated R code for EWA - EWC strategy&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;#Note: Try to put everything in a data-frame
lapply(c(&amp;quot;zoo&amp;quot;, &amp;quot;tidyr&amp;quot;, &amp;quot;plyr&amp;quot;, &amp;quot;dplyr&amp;quot;,
         &amp;quot;gtools&amp;quot;,&amp;quot;googlesheets&amp;quot;, &amp;quot;quantmod&amp;quot;, 
         &amp;quot;urca&amp;quot;, &amp;quot;PerformanceAnalytics&amp;quot;, &amp;quot;parallel&amp;quot;), require, character.only = T)

source(&#39;util/calculateReturns.R&#39;)
source(&#39;util/calculateMaxDD.R&#39;)
source(&#39;util/backshift.R&#39;)
source(&#39;util/extract_stock_prices.R&#39;)
source(&#39;util/cointegration_pair.R&#39;)


#Reading in the data
df = read.csv(&#39;kalman_filter/inputData_EWA_EWC.csv&#39;)
df = subset(df, select = c(&amp;quot;Date&amp;quot;, &amp;quot;EWC&amp;quot;, &amp;quot;EWA&amp;quot;))

# Augment x with ones to  accomodate possible offset in the regression between y vs x.
# df$EWA_ones = 1

# delta=1 gives fastest change in beta, delta=0.000....1 allows no change (like traditional linear regression).
delta = 0.0001 

#yhat=np.full(y.shape[0], np.nan) # measurement prediction
df$yhat = NA

#Initialize matrix
df$e = df$yhat # e = yhat.copy(), residuals
df$Q = df$yhat # Q = yhat.copy(), measurement variance

# For clarity, we denote R(t|t) by P(t). Initialize R, P and beta.
R = matrix(dat = rep(0, 4), nrow = 2, ncol = 2) #R = np.zeros((2,2))
P = R   #P = R.copy()

#Store beta in df and separately for computation
beta = matrix(dat = rep(NA, nrow(df) * 2), nrow = 2, ncol = nrow(df))
df$beta1 = NA; df$beta2 = NA  #beta = np.full((2, x.shape[0]), np.nan)

Vw = delta/(1-delta) * diag(2) #Vw=delta/(1-delta)*np.eye(2)
Ve = 0.001

# Initialize beta(:, 1) to zero
beta[, 1] = 0 #beta[:, 0]=0
df$beta1 = beta[1, 1]; df$beta2 = beta[2, 1]

#for t in range(len(y)):
for (t in 1:nrow(df)){
 
  if(t &amp;gt; 1){
    #Update matrix
    beta[, t] = beta[, t-1]
    R = P + Vw
    
    #Update df
    df$beta1[t] = beta[1, t]
    df$beta2[t] = beta[2, t]
  }
  
  # yhat[t, ] = as.matrix(x[t, ]) %*% as.matrix(beta[, t])    #yhat[t]=np.dot(x[t, :], beta[:, t])
  df$yhat[t] = as.matrix(data.frame(df$EWA[t], 1)) %*% as.matrix(beta[, t]) 
  
  # Q[t, ] = (as.matrix(x[t, ]) %*% R) %*% t(as.matrix(x[t, ])) + Ve  #Q[t] = np.dot(np.dot(x[t, :], R), x[t, :].T)+Ve
  df$Q[t] = (as.matrix(data.frame(df$EWA[t], 1)) %*% R) %*% t(as.matrix(data.frame(df$EWA[t], 1))) + Ve
  
  # e[t, ] = y[t, ] - yhat[t, ] #e[t]=y[t]-yhat[t] # measurement prediction error
  df$e[t] = df$EWC[t] - df$yhat[t]
  
  K = R %*% t(as.matrix(data.frame(df$EWA[t], 1))) / df$Q[t]  #K = np.dot(R, x[t, :].T)/Q[t] #  Kalman gain
  beta[, t] = beta[, t] + K %*% as.matrix(df$e[t]) #beta[:, t]=beta[:, t]+np.dot(K, e[t]) #  State update. Equation 3.11
  
  #Update df
  df$beta1[t] = beta[1, t]
  df$beta2[t] = beta[2, t]
  
  # State covariance update. Euqation 3.12
  P = R - ((K %*% as.matrix(data.frame(df$EWA[t], 1))) %*% R) #P = R-np.dot(np.dot(K, x[t, :]), R) 
}

#Generated signals
df$Q_root = df$Q ^ 0.5
df$longs &amp;lt;- df$e &amp;lt;= -df$Q ^ 0.5 # buy spread when its value drops below 2 standard deviations.
df$shorts &amp;lt;- df$e &amp;gt;= df$Q ^ 0.5  # short spread when its value rises above 2 standard deviations.  Short EWC

df$longExits  &amp;lt;- df$e &amp;gt; 0 
df$shortExits &amp;lt;- df$e &amp;lt; 0

# initialize to 0
df$numUnitsLong = NA
df$numUnitsShort = NA
df$numUnitsLong[0]=0.
df$numUnitsShort[0]=0.

df$numUnitsLong[df$longs]=1.
df$numUnitsLong[df$longsExit]=0
df$numUnitsLong = ifelse(is.na(df$numUnitsLong), 0, df$numUnitsLong)

df$numUnitsShort[df$shorts]=-1.
df$numUnitsShort[df$shortsExit]=0
df$numUnitsShort = ifelse(is.na(df$numUnitsShort), 0, df$numUnitsShort)

df$numUnits = df$numUnitsLong + df$numUnitsShort 

df$position1 = 0; df$position2 = 0 

df$position1 = ifelse(df$numUnits == -1, -1, df$position1)   #short EWC, Long EWA --&amp;gt; df$e[t] = df$EWC[t] - df$yhat[t]
df$position2 = ifelse(df$numUnits == -1, 1, df$position2)  #short EWC, Long EWA 

df$position1 = ifelse(df$numUnits == 1, 1, df$position1)   #long EWC, short EWA 
df$position2 = ifelse(df$numUnits == 1, -1, df$position2)  #long EWC, short EWA 

# df$positions = data.frame(df$numUnits, df$numUnits) * (data.frame(-df$beta1, 1)) * data.frame(df$EWA, df$EWC)   #Adjusted price
df$positions = data.frame(df$numUnits, df$numUnits) * (data.frame(1, -df$beta1)) * data.frame(df$EWC, df$EWA)   #Adjusted price

#Returns
df$dailyret1 &amp;lt;- c(NA, (df$EWC[2: nrow(df)] - df$EWC[1: (nrow(df) - 1)])/df$EWC[1: (nrow(df) - 1)])
df$dailyret2 &amp;lt;- c(NA, (df$EWA[2: nrow(df)] - df$EWA[1: (nrow(df) - 1)])/df$EWA[1: (nrow(df) - 1)])

#Daily returns
# lag(df$position1, 1)
# lag(df$position2, 1) * df$beta1
df$pnl = lag(df$positions$df.numUnits, 1) * df$dailyret1  + lag(df$positions$df.numUnits.1, 1) * df$dailyret2

df$ret = (df$pnl)/lag((df$positions$df.numUnits + df$positions$df.numUnits.1), 1)
df$ret = ifelse(is.na(df$ret), 0, df$ret)
df$ret[2] = 0

#Sharpe ratio
sqrt(252)*mean(df$pnl, na.rm = TRUE)/sd(df$pnl, na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>How I Find Country Pairs for Mean Reversion Strategy</title>
      <link>/post/how-i-find-country-pairs-for-mean-reversion-strategy/</link>
      <pubDate>Wed, 26 Dec 2018 12:30:03 +0800</pubDate>
      
      <guid>/post/how-i-find-country-pairs-for-mean-reversion-strategy/</guid>
      <description>

&lt;h2 id=&#34;how-i-find-country-pairs-for-mean-reversion-strategy&#34;&gt;How I Find Country Pairs for Mean Reversion Strategy&lt;/h2&gt;

&lt;p&gt;As mentioned in my previous post &lt;a href=&#34;https://jirong-huang.netlify.com/post/research-to-production-pipeline-for-mean-reversion/&#34;&gt;here&lt;/a&gt;, the first step for a mean reversion strategy is to conduct some background quantitative research.&lt;/p&gt;

&lt;h3 id=&#34;step-1&#34;&gt;Step 1&lt;/h3&gt;

&lt;p&gt;First, I use a pair trading function to loop across 800+ country pairs (created from combination function),&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pair_trading = function(stock1, stock2, trade_amount, finance_rates, start_date, end_date, 
                        prop_train, enter_z_score, exit_z_score){

## More codes here
   
## Return this
key_info = list(
  ticker = c(stock1, stock2),
  start_date = start_date,
  trade_table = data_trade,
  sharpe = c(sharpeRatioTrainset, sharpeRatioTestset),
  half_life = half_life,
  profits = data_trade_stats,
  max_drawdown = c(table.DownsideRisk(data$pnl[trainset])[1]$pnl[7], table.DownsideRisk(data$pnl[testset])[1]$pnl[7]),
  returns = cbind(table.AnnualizedReturns(data$pnl[trainset]), table.AnnualizedReturns(data$pnl[testset])),
  hedgeRatio_mean_sd = c(as.numeric(hedgeRatio), as.numeric(data_trade$spreadMean[nrow(data_trade)]), as.numeric(data_trade$spreadStd[nrow(data_trade)])),   #critical --&amp;gt;to be used in real-time trading
  close_z_score = as.numeric(data_trade$zscore[nrow(data_trade)]),
  hist_spread = data_trade$spread[(nrow(data_trade) - round(half_life) + 2):nrow(data_trade)],
  prop_days_mkt = c(prop_days_mkt_train, prop_days_mkt_test),
  close_price = c(data_trade$Close[nrow(data_trade)], data_trade$Close.1[nrow(data_trade)]),
  win_rate = c(perc_win_train, perc_win_test)
  # ,
  # chart_train = charts.PerformanceSummary(data$pnl[trainset]),
  # chart_test = charts.PerformanceSummary(data$pnl[testset])
)

return(key_info)
                        
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-2&#34;&gt;Step 2&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Next, I select pairs with sharpe ratio &amp;gt;1 in both training and testing periods.&lt;/li&gt;
&lt;li&gt;And also select pairs with shorter half-life i.e. shorter duration before it reverts to its mean path - more than 5 and lesser than 25&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;step-3&#34;&gt;Step 3&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Then I went on to IShares website to get the respective tickers&amp;rsquo; industries&amp;rsquo; composition.
&lt;img src=&#34;/post/img/country_composition.png&#34; alt=&#34;/post/img/country_composition.png&#34;&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;With this new piece of information, I went on to compute the manhattan distance, euclidean distance and correlation between these country pairs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Next I applied percentile ranks to these distance measures and find an average percentile rank&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Anything that is above 50th percentile is selected.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;distance_metrics = function(stock1, stock2){
  
  dist = c(NA, NA, NA)
  
  tryCatch({
  
  ctry_pair_composition_sub = subset(ctry_pair_composition, ctry_pair_composition$ticker == stock1 | ctry_pair_composition$ticker == stock2)
  manhattan = as.numeric(distance(ctry_pair_composition_sub[, -1], method = &amp;quot;manhattan&amp;quot;))
  euclidean = as.numeric(distance(ctry_pair_composition_sub[, -1], method = &amp;quot;euclidean&amp;quot;))
  correlation = cor(as.numeric(ctry_pair_composition_sub[1, -1]), as.numeric(ctry_pair_composition_sub[2, -1]))
  
  dist = c(manhattan, euclidean, correlation)
  }, error=function(e){})
  
  return(dist)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And ta-dah! This is the final selected country pairs that I will be using for my mean reversion strategy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/final_selection.png&#34; alt=&#34;/post/img/final_selection.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;further-improvement&#34;&gt;Further improvement&lt;/h3&gt;

&lt;p&gt;Note: I could have applied co-integration test. Will do it pretty soon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Research to Production Pipeline for Mean Reversion</title>
      <link>/post/research-to-production-pipeline-for-mean-reversion/</link>
      <pubDate>Tue, 25 Dec 2018 18:07:19 +0800</pubDate>
      
      <guid>/post/research-to-production-pipeline-for-mean-reversion/</guid>
      <description>

&lt;h2 id=&#34;research-to-production-pipeline-for-mean-reversion&#34;&gt;Research to Production Pipeline for Mean Reversion&lt;/h2&gt;

&lt;p&gt;Here is a high level overview of something that I&amp;rsquo;m working on.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been grappling with the finite state automata Event Driven Computing transitions and I kinda sorted it out for production use.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/research_to_production.png&#34; alt=&#34;/post/img/research_to_production.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prototype Pair Trading Strategy for Silver ETFs</title>
      <link>/post/prototype-of-pair-trading-strategy-for-silver-etfs/</link>
      <pubDate>Tue, 18 Dec 2018 13:03:44 +0800</pubDate>
      
      <guid>/post/prototype-of-pair-trading-strategy-for-silver-etfs/</guid>
      <description>&lt;p&gt;In these 2 weeks, I&amp;rsquo;ll deploy my pair trading algo strategy into my server.&lt;/p&gt;

&lt;p&gt;I modified the code below from a renowned quant trader, Ernest Chan. The basic idea is to find z-scores through moving average &amp;amp; moving SD of spread. If it&amp;rsquo;s more than absolute of z-score, I will either short or long the spread depending on the polarity.&lt;/p&gt;

&lt;p&gt;In the backtesting below (using a pair of silver ETFs as an example), I assumed a hypothetical amount of 10,000 dollars per trade.&lt;/p&gt;

&lt;p&gt;Results are pretty good with a healthy sharpe ratio of 2.7 in the training set and 1.6 in the testing set of data. Annualized return is approximately 26% (translates to 2600 dollars) for the test set.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/equity_curve.png&#34; alt=&#34;/post/img/equity_curve.png&#34;&gt;
&lt;img src=&#34;/post/img/summary_stats.png&#34; alt=&#34;/post/img/summary_stats.png&#34;&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rm(list=ls()) # clear workspace

library(&#39;zoo&#39;)
library(&amp;quot;tidyr&amp;quot;)
library(&amp;quot;dplyr&amp;quot;)
require(&amp;quot;quantmod&amp;quot;)
require(&amp;quot;urca&amp;quot;)
require(&amp;quot;PerformanceAnalytics&amp;quot;)
source(&#39;R/util/calculateReturns.R&#39;)
source(&#39;R/util/calculateMaxDD.R&#39;)
source(&#39;R/util/backshift.R&#39;)
source(&#39;R/util/extract_stock_prices.R&#39;)

#List of silver etfs, SIVR, USV, SLV, DBS
stock1 = &amp;quot;SIVR&amp;quot;
stock2 = &amp;quot;USV&amp;quot;

start_date = &amp;quot;2014-12-30&amp;quot;
end_date = &amp;quot;2018-12-30&amp;quot;

prop_train = 0.65
enter_z_score = 2     #Can use nlmb to vary
exit_z_score = 1     #Can use nlmb to vary

trade_amount = 10000
finance_rates = 2.5/100

data1 = df_crawl_time_series(stock1, start_date, end_date)
data1 = subset(data1, select = c(&amp;quot;Date&amp;quot;, &amp;quot;Open&amp;quot;, &amp;quot;Close&amp;quot;))
data1$Date = as.Date(data1$Date)

data2 = df_crawl_time_series(stock2, start_date, end_date)
data2 = subset(data2, select = c(&amp;quot;Date&amp;quot;, &amp;quot;Open&amp;quot;, &amp;quot;Close&amp;quot;))
data2$Date = as.Date(data2$Date)

data1 = xts(data1[, -1], order.by = data1[, 1])
data2 = xts(data2[, -1], order.by = data2[, 1])

data = merge(data1, data2)
data = as.data.frame(data)
data = subset(data, !is.na(data$Close) &amp;amp; !is.na(data$Close.1))

#  define indices for training and test sets
trainset &amp;lt;- 1:as.integer(nrow(data) * prop_train)
testset &amp;lt;- (length(trainset)+1):nrow(data)

#Cointegration test--&amp;gt;See if test of r&amp;lt;=1 &amp;gt; threshold. If more cointegrating
jotest=ca.jo(data.frame(data$Close[trainset], data$Close.1[trainset]), type=&amp;quot;trace&amp;quot;, K=2, ecdet=&amp;quot;none&amp;quot;, spec=&amp;quot;longrun&amp;quot;)
summary(jotest)

is_coint = jotest@teststat[1] &amp;gt; jotest@cval[1,3]
if(is_coint){
  print(&amp;quot;This pair&#39;s training set is cointegrating&amp;quot;)
}else{
  print(&amp;quot;This pair&#39;s training set is not cointegrating&amp;quot;)  
}

#Hedge ratio
result &amp;lt;- lm(data$Close[trainset] ~ 0 + data$Close.1[trainset])
hedgeRatio &amp;lt;- coef(result) # 1.631

#Spread
data$spread &amp;lt;- data$Close - hedgeRatio * data$Close.1

##########################Calculate half life#############################
# Calculate half life of mean reversion (residuals)
# Calculate yt-1 and (yt-1-yt)
# pull residuals to a vector
spread_train = data$spread[trainset]
y.lag &amp;lt;- c(spread_train[2:length(spread_train)], 0) # Set vector to lag -1 day
y.lag &amp;lt;- y.lag[1:length(y.lag)-1] # As shifted vector by -1, remove anomalous element at end of vector
spread_train &amp;lt;- spread_train[1:length(spread_train)-1] # Make vector same length as vector y.lag
y.diff &amp;lt;- spread_train - y.lag # Subtract todays close from yesterdays close
y.diff &amp;lt;- y.diff [1:length(y.diff)-1] # Make vector same length as vector y.lag
prev.y.mean &amp;lt;- y.lag - mean(y.lag, na.rm = T) # Subtract yesterdays close from the mean of lagged differences
prev.y.mean &amp;lt;- prev.y.mean [1:length(prev.y.mean )-1] # Make vector same length as vector y.lag
final.df &amp;lt;- data.frame(y.diff,prev.y.mean) # Create final data frame

# Linear Regression With Intercept
result &amp;lt;- lm(y.diff ~ prev.y.mean, data = final.df)
half_life &amp;lt;- -log(2)/coef(result)[2]   #Looking at this to 

if(half_life &amp;lt; 3){
  half_life = 14
}

######################MA of Spread#################################
#Change this to half life for lookback--&amp;gt;https://flare9xblog.com/2017/11/02/pairs-trading-testing-for-conintergration-adf-johansen-test-half-life-of-mean-reversion/
#Try EMA too
data$spread = zoo::na.locf(data$spread)
data$spreadMean &amp;lt;- SMA(data$spread, round(half_life))
data$spreadStd &amp;lt;- runSD(data$spread, n = round(half_life), sample = TRUE, cumulative = FALSE)

# data$spreadMean &amp;lt;- mean(data$spread[trainset], na.rm = T)
# data$spreadStd &amp;lt;- sd(data$spread[trainset], na.rm = T)

data$zscore = (data$spread - data$spreadMean)/data$spreadStd

data$longs &amp;lt;- data$zscore &amp;lt;= -enter_z_score # buy spread when its value drops below 2 standard deviations.
data$shorts &amp;lt;- data$zscore &amp;gt;= enter_z_score # short spread when its value rises above 2 standard deviations.

#  exit any spread position when its value is within 1 standard deviation of its mean.
data$longExits   &amp;lt;- data$zscore &amp;gt;= -exit_z_score 
data$shortExits &amp;lt;- data$zscore &amp;lt;= exit_z_score 

#Signal
data$posL1 = NA
data$posL2 = NA
data$posS1 = NA
data$posS2 = NA

# initialize to 0
data$posL1[1] &amp;lt;- 0; data$posL2[1] &amp;lt;- 0
data$posS1[1] &amp;lt;- 0; data$posS2[1] &amp;lt;- 0

data$posL1[data$longs] &amp;lt;- 1
data$posL2[data$longs] &amp;lt;- -1

data$posS1[data$shorts] &amp;lt;- -1
data$posS2[data$shorts] &amp;lt;- 1

data$posL1[data$longExits] &amp;lt;- 0
data$posL2[data$longExits] &amp;lt;- 0
data$posS1[data$shortExits] &amp;lt;- 0
data$posS2[data$shortExits] &amp;lt;- 0

#positions
data$posL1 &amp;lt;- zoo::na.locf(data$posL1); data$posL2 &amp;lt;- zoo::na.locf(data$posL2)
data$posS1 &amp;lt;- zoo::na.locf(data$posS1); data$posS2 &amp;lt;- zoo::na.locf(data$posS2)
data$position1 &amp;lt;- data$posL1 + data$posS1
# data$position1 = -data$position1    #Don&#39;t know why. It should be flipped!!!

data$position2 &amp;lt;- data$posL2 + data$posS2
# data$position2 = -data$position2    #Don&#39;t know why. It should be flipped!!!

#Returns
data$dailyret1 &amp;lt;- ROC(data$Close) #  last row is [385,] -0.0122636689 -0.0140365802
data$dailyret2 &amp;lt;- ROC(data$Close.1) #  last row is [385,] -0.0122636689 -0.0140365802

#Backshifting here. But signal is for following day returns!. So can still use latest Z-score
data$date = as.Date(row.names(data))
data = xts(data[,-which(names(data) == &amp;quot;date&amp;quot;)], order.by = data[, which(names(data) == &amp;quot;date&amp;quot;)])

data$pnl = lag(data$position1, 1) * data$dailyret1  + lag(data$position2, 1) * data$dailyret2

#Sharpe ratio
sharpeRatioTrainset &amp;lt;- sqrt(252)*mean(data$pnl[trainset], na.rm = TRUE)/sd(data$pnl[trainset], na.rm = TRUE)
sharpeRatioTrainset

sharpeRatioTestset &amp;lt;- sqrt(252)*mean(data$pnl[testset], na.rm = TRUE)/sd(data$pnl[testset], na.rm = TRUE)
sharpeRatioTestset 

#Performance analytics
charts.PerformanceSummary(data$pnl[testset])
table.Drawdowns(data$pnl[testset])
table.DownsideRisk(data$pnl[testset])
table.AnnualizedReturns(data$pnl[testset])

#Number of days not in the market
sum(data$pnl == 0, na.rm = T)/length(data$pnl)

#Putting a trade indicator
data$trade_indicator = lag(ifelse(data$position2 != 0 &amp;amp; !is.na(data$position2), 1, 0))

#Putting a unique id
count = 0
data$trade_id = NA

for(i in 2:nrow(data)){ 
  if(as.numeric(data$trade_indicator[i-1]) == 0 &amp;amp; as.numeric(data$trade_indicator[i]) != 0){
    count = count + 1
    data$trade_id[i] = count
  }else if(as.numeric(data$trade_indicator[i-1]) != 0 &amp;amp; as.numeric(data$trade_indicator[i]) != 0){
    data$trade_id[i] = count
  }
}

#Simple trade statistics
data$test = 0; data$test[testset] = 1 
data$pnl_add1 = data$pnl + 1
data_trade = as.data.frame(data)
data_trade_stats = data_trade %&amp;gt;%
  group_by(trade_id, test) %&amp;gt;%
  summarize(trade_duration = n(),
            cum_pnl = prod(pnl_add1, na.rm = T))

data_trade_stats$cum_pnl = data_trade_stats$cum_pnl - 1
data_trade_stats$profit_per_trade = data_trade_stats$cum_pnl * trade_amount

#Financing charges --&amp;gt;Depends on length of days
data_trade_stats$finance_fees =  trade_amount * finance_rates * (data_trade_stats$trade_duration/365)

#Commission fees
data_trade_stats$comm_fess = 4  #2 for 1 pair

#Net profit
data_trade_stats$profit_per_trade_less_comms = data_trade_stats$profit_per_trade - data_trade_stats$finance_fees - data_trade_stats$comm_fess

#Average loss
data_trade_stats = data_trade_stats[-which(is.na(data_trade_stats)), ]

data_trade_stats %&amp;gt;%
  group_by(test) %&amp;gt;%
  summarize(sum_profits = sum(profit_per_trade_less_comms), 
            mean_profits = mean(profit_per_trade_less_comms),
            na.rm = T)

sum(data_trade_stats$profit_per_trade_less_comms &amp;lt; 0)/ nrow(data_trade_stats)

summary(data_trade_stats$profit_per_trade_less_comms)

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Summary of My Computational Photography Module From Georgia Tech Computer Science Masters</title>
      <link>/post/summary-of-my-computational-photography-from-georgia-tech-computer-science-masters/</link>
      <pubDate>Sat, 08 Dec 2018 01:11:52 +0800</pubDate>
      
      <guid>/post/summary-of-my-computational-photography-from-georgia-tech-computer-science-masters/</guid>
      <description>&lt;p&gt;For what&amp;rsquo;s worth, here is a summary of what I went through for my Georgia Tech Computer Science Msc Computational Photography module.&lt;/p&gt;

&lt;p&gt;And it&amp;rsquo;s really painful but rewarding!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/CP_1.png&#34; alt=&#34;/post/img/CP_1.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_2.png&#34; alt=&#34;/post/img/CP_2.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_3.png&#34; alt=&#34;/post/img/CP_3.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_4.png&#34; alt=&#34;/post/img/CP_4.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_5.png&#34; alt=&#34;/post/img/CP_5.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_6.png&#34; alt=&#34;/post/img/CP_6.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_7.png&#34; alt=&#34;/post/img/CP_7.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_8.png&#34; alt=&#34;/post/img/CP_8.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_9.png&#34; alt=&#34;/post/img/CP_9.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Colorization</title>
      <link>/post/colorization/</link>
      <pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/colorization/</guid>
      <description>

&lt;h2 id=&#34;colorization&#34;&gt;Colorization&lt;/h2&gt;

&lt;p&gt;The following is a high level project pipeline of my Computational Photography Colorization report. The project scope involves minimizing a quadratic cost function. An artist would only need to make a few colour scribble on a grey photograph and the algorithm will automatically populate the entire photograph with the associated colours.&lt;/p&gt;

&lt;p&gt;1.Input: I first read in the image using imread function.&lt;/p&gt;

&lt;p&gt;2.Find the difference: Next I compute the difference between the marked and grey scale image. This would feed into step 5.&lt;/p&gt;

&lt;p&gt;3.Transform to YIQ space: Then I convert the grey image and the marked version from RGB space to YIQ space 2 &amp;amp; 3 . I wrote a function, rgbToyiq in color_space.py to convert rgb dimension to that of YIQ.&lt;/p&gt;

&lt;p&gt;4.Compute weight matrix:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The next step, also the most complicated one is to compute the weight matrix.&lt;/li&gt;
&lt;li&gt;I first initialize 3 matrices of size height X width X size of window (9): row indices (i, j count), colIndices and values (weights) to hold key information during the loop&lt;/li&gt;
&lt;li&gt;The algo will loop through each pixel. And it will compute the weights (using marked) according to formula below in a window of size 9 i.e. 9 pixels in a window (including the pixel).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;5.Solve Ax = B: Once the weights are obtained, I proceed to obtain a least square solution.&lt;/p&gt;

&lt;p&gt;6.Lastly, I transform the YIQ output back to RGB space.&lt;/p&gt;

&lt;p&gt;Here are the photographs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/baby.bmp&#34; alt=&#34;/post/img/baby.bmp&#34;&gt;
&lt;img src=&#34;/post/img/baby_marked.bmp&#34; alt=&#34;/post/img/baby_marked.bmp&#34;&gt;
&lt;img src=&#34;/post/img/baby_colorized.bmp&#34; alt=&#34;/post/img/baby_colorized.bmp&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Architecture and Process Flow for My Algorithmic Trading</title>
      <link>/post/architecture-and-process-flow-for-my-algorithmic-trading/</link>
      <pubDate>Sun, 04 Nov 2018 10:19:13 +0800</pubDate>
      
      <guid>/post/architecture-and-process-flow-for-my-algorithmic-trading/</guid>
      <description>

&lt;h2 id=&#34;project-that-i-will-be-working-in-2018-2019&#34;&gt;Project that I will be working in 2018-2019&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/mvp_algo_trading.png&#34; alt=&#34;/post/img/mvp_algo_trading.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Seam Carving</title>
      <link>/post/seam-carving/</link>
      <pubDate>Thu, 25 Oct 2018 13:23:23 +0800</pubDate>
      
      <guid>/post/seam-carving/</guid>
      <description>

&lt;h2 id=&#34;snippet-of-my-seam-carving-report-from-my-msc-computer-science-georgia-tech-s-computational-photography-module&#34;&gt;Snippet of my Seam Carving Report from my Msc Computer Science Georgia Tech&amp;rsquo;s Computational Photography module&lt;/h2&gt;

&lt;p&gt;Besides removing of streams, we can also add streams. We identify k streams for removal and duplicate by averaging the left and right neighbours. The computation of these averages is done by convolving the following matrix with the imagesâ€™ colour channels.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kernel = np.array([[0, 0, 0],
         [0.5, 0, 0.5],
         [0, 0, 0]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the implementation of my scaling_up algorithm, I first remove k streams (depending on ratio set by user) and recorded the coordinates and cumulative energy values of the original picture in each removal.&lt;/p&gt;

&lt;p&gt;Then I reverse the whole process by adding the stream back together with the averaged values of neighbours&lt;/p&gt;

&lt;p&gt;I implemented this scaling_up algorithn for the dolphin pictures.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;8(a) is the original picture&lt;/li&gt;
&lt;li&gt;8&amp;copy; Enlarged picture with added streams: python main.py fig8 u c 1.5 y&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;8(d) Enalrged picture without added streams: python main.py fig8 u c 1.5 n&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;8(f) Enlarged picture with scaling up algorithm implemented twice: python main.py fig8_processed u c 1.5 n&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Figure 8(a), 8&amp;copy;, 8(d), (f)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/seam_carving.img.png&#34; alt=&#34;/post/img/seam_carving.img.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R package: Decomposing a Position Into Exchange Rate and Non Exchange Rate Effects</title>
      <link>/post/decomposing-a-position-into-exchange-rate-and-non-exchange-rate-effects/</link>
      <pubDate>Thu, 25 Oct 2018 11:39:24 +0800</pubDate>
      
      <guid>/post/decomposing-a-position-into-exchange-rate-and-non-exchange-rate-effects/</guid>
      <description>

&lt;h2 id=&#34;decomposing-a-position-into-exchange-rate-and-non-exchange-rate-effects&#34;&gt;Decomposing a Position Into Exchange Rate and Non Exchange Rate Effects&lt;/h2&gt;

&lt;p&gt;If you are someone with a stake in foreign positions, this package I wrote here may be a useful tool to help you understand the impact of foreign currency on your positions. For instance,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If you are an investor, you may use it to analyze impact of exchange rate on your investment positions.&lt;/li&gt;
&lt;li&gt;If you are in the treasury department, you may wish to analyze the impact of exchange rates on your bonds.&lt;/li&gt;
&lt;li&gt;If you are in the finance department, you could analyze the exchange rate impact on your foreign revenue.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To start using this package, you may first install the devtools package and execute the following command. install_github(&amp;ldquo;jironghuang/RemoveExchangeRateEffects&amp;rdquo;). The R documentation is as follows,&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/exch_documentation1.png&#34; alt=&#34;/post/img/exch_documentation1.png&#34;&gt;
&lt;img src=&#34;/post/img/exch_documentation2.png&#34; alt=&#34;/post/img/exch_documentation2.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;You may follow the 2 examples to better understand how this package works.&lt;/p&gt;

&lt;h3 id=&#34;example-1&#34;&gt;Example 1&lt;/h3&gt;

&lt;p&gt;In summary, what the example does below is to decompose 1 instrument position in SGD (column value) - from the perspective of someone staying in Singapore - into local static value (i.e if I keep the exchange rate constant at the start of the period) and the residual exchange rate impact.&lt;/p&gt;

&lt;p&gt;If you look at the value at the end of the period (Oct 2018), you would notice that the value in SGD fell from 331 to 261. From the perspective of a Singaporean local - through this package -  we can understand that the appreciation in USD negate the fall in value by 4 SGD.&lt;/p&gt;

&lt;p&gt;If you are an economist, you could have considered the exchange rate elasticities. But let&amp;rsquo;s ignore that for now.&lt;/p&gt;

&lt;h3 id=&#34;quick-example-1-in-r-codes-decomposing-a-single-instrument&#34;&gt;Quick example 1 in R codes (decomposing a single instrument)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;library(devtools)
install_github(&amp;quot;jironghuang/RemoveExchangeRateEffects&amp;quot;)
library(RemoveExchangeRateEffects)

sp_exch_rate_pair = &amp;quot;USDSGD=X&amp;quot;  #exchange rate pair. e.g &amp;quot;USDSGD=X&amp;quot;. &amp;quot;&amp;lt;Foreign_currency&amp;gt;&amp;lt;local_currency&amp;gt;=X&amp;quot;

ap_start_date &amp;lt;- as.Date(&amp;quot;2017-10-01&amp;quot;)  #starting date of portfolio e.g. 2017-10-01
ap_end_date &amp;lt;- as.Date(&amp;quot;2020-10-01&amp;quot;) #ending date of portfolio e.g. 2020-10-01. If you include a date beyond current date, the function will use the current date instead
np_mthly_yearly = &amp;quot;monthly&amp;quot;  #alternatively this could be &amp;quot;yearly&amp;quot;&amp;quot;

data(eg_dat) #example dataset that I included in this package
dp_dates_investment_value = instrument
o_exchRate_effect &amp;lt;- exchange_rate_decomposition(sp_exch_rate_pair, ap_start_date, ap_end_date, np_mthly_yearly, dp_dates_investment_value)
o_exchRate_effect$get_portfolio()

    value_in_sgd exchange_rate fgn_value local_static_value exch_rate_impact
Oct 2017 331.53   1.36010  243.7541           331.5300        0.0000000
Nov 2017 308.85   1.34670  229.3384           311.9231       -3.0731344
Dec 2017 311.35   1.33780  232.7328           316.5399       -5.1899425
Jan 2018 354.31   1.31168  270.1192           367.3892      -13.0791734
Feb 2018 343.06   1.32433  259.0442           352.3260       -9.2660108
Mar 2018 266.13   1.31090  203.0132           276.1183       -9.9882495
Apr 2018 293.90   1.32577  221.6825           301.5104       -7.6103599
May 2018 284.73   1.33850  212.7232           289.3248       -4.5948212
Jun 2018 342.95   1.36830  250.6395           340.8948        2.0552438
Jul 2018 298.14   1.36140  218.9952           297.8553        0.2846937
Aug 2018 301.66   1.36700  220.6730           300.1374        1.5226438
Sep 2018 264.77   1.36732  193.6416           263.3719        1.3980921
Oct 2018 260.95   1.38061  189.0107           257.0734        3.8766087

o_exchRate_effect$get_diff_portfolio_value()
[1] 3.8766087
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;example-2&#34;&gt;Example 2&lt;/h3&gt;

&lt;p&gt;The second example builds upon the first. I&amp;rsquo;ve expanded the previous function to decompose multiple instruments at once.&lt;/p&gt;

&lt;p&gt;get_full_decomposition() returns a list of data frames with the decompositions.&lt;/p&gt;

&lt;p&gt;But before you implement the function above, you would have to add the information via the mutator functions as shown in the example below (the functions with the prefix set)&lt;/p&gt;

&lt;h3 id=&#34;quick-example-2-in-r-codes-decomposing-multiple-instruments-at-1-time&#34;&gt;Quick example 2 in R codes (Decomposing multiple instruments at 1 time)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;data(eg_dat)

er = c(&amp;quot;USDSGD=X&amp;quot;, &amp;quot;GBPSGD=X&amp;quot;)
start_date = c(&amp;quot;2017-10-01&amp;quot;, &amp;quot;2017-10-01&amp;quot;)
end_date = c(&amp;quot;2020-10-01&amp;quot;, &amp;quot;2020-10-01&amp;quot;)
freq = c(&amp;quot;monthly&amp;quot;, &amp;quot;monthly&amp;quot;)
dat = list(eg_dat, eg_dat)


o_exchRate_effect &amp;lt;- multiple_exchange_rate_decomposition(2)
o_exchRate_effect$set_sa_exch_rate_pair(er) #adding an array of exchange rate pairs
o_exchRate_effect$set_sa_start_date(start_date) #adding an array of starting dates
o_exchRate_effect$set_sa_end_date(end_date) #adding an array of ending dates
o_exchRate_effect$set_sa_mthly_yearly(freq) #adding an array of &amp;quot;monthly&amp;quot; or &amp;quot;yearly&amp;quot; option
o_exchRate_effect$set_dl_dates_investment_value(dat) #adding list of data frames
o_exchRate_effect$get_full_decomposition()  #carry out decomposition and obtain a list of data frames

[[1]]
          value exchange_rate fgn_value local_static_value exch_rate_impact
Oct 2017 331.53       1.36010  243.7541           331.5300        0.0000000
Nov 2017 308.85       1.34670  229.3384           311.9231       -3.0731344
Dec 2017 311.35       1.33780  232.7328           316.5399       -5.1899425
Jan 2018 354.31       1.31168  270.1192           367.3892      -13.0791734
Feb 2018 343.06       1.32433  259.0442           352.3260       -9.2660108
Mar 2018 266.13       1.31090  203.0132           276.1183       -9.9882495
Apr 2018 293.90       1.32577  221.6825           301.5104       -7.6103599
May 2018 284.73       1.33850  212.7232           289.3248       -4.5948212
Jun 2018 342.95       1.36830  250.6395           340.8948        2.0552438
Jul 2018 298.14       1.36140  218.9952           297.8553        0.2846937
Aug 2018 301.66       1.36700  220.6730           300.1374        1.5226438
Sep 2018 264.77       1.36732  193.6416           263.3719        1.3980921
Oct 2018 260.95       1.38061  189.0107           257.0734        3.8766087

[[2]]
          value exchange_rate fgn_value local_static_value exch_rate_impact
Oct 2017 331.53       1.79703  184.4877           331.5300        0.0000000
Nov 2017 308.85       1.80690  170.9281           307.1629        1.6870605
Dec 2017 311.35       1.79812  173.1531           311.1613        0.1887369
Jan 2018 354.31       1.85680  190.8175           342.9048       11.4051640
Feb 2018 343.06       1.84162  186.2816           334.7537        8.3062984
Mar 2018 266.13       1.83911  144.7059           260.0408        6.0892228
Apr 2018 293.90       1.82588  160.9635           289.2562        4.6437963
May 2018 284.73       1.77878  160.0704           287.6513       -2.9212846
Jun 2018 342.95       1.78907  191.6918           344.4759       -1.5258666
Jul 2018 298.14       1.78638  166.8962           299.9175       -1.7774444
Aug 2018 301.66       1.77860  169.6053           304.7858       -3.1258259
Sep 2018 264.77       1.78285  148.5094           266.8759       -2.1058633
Oct 2018 260.95       1.76900  147.5127           265.0848       -4.1347817

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Shift Share Analysis Package I developed</title>
      <link>/post/shift-share-analysis-package/</link>
      <pubDate>Sat, 22 Sep 2018 12:23:28 +0800</pubDate>
      
      <guid>/post/shift-share-analysis-package/</guid>
      <description>

&lt;h2 id=&#34;shift-share-analysis-package-i-developed&#34;&gt;Shift-share Analysis Package I developed&lt;/h2&gt;

&lt;p&gt;During my career, I often have to deal with compositional &amp;amp; within group effects. For instance, the employment rate fell by 3% across 2 period. How much of it is due to an increase in employment rate within the sub-group and how much of it is due to compositional shift (for example ageing population).&lt;/p&gt;

&lt;p&gt;A formal way to explain these effects is known as shift-share analysis. It allows you to decompose percentage point change or absolute changes (WIP) into within group and across group effects.&lt;/p&gt;

&lt;p&gt;A package currently used in the R community is REAT, but it doesn&amp;rsquo;t allow you to decompose the effects into finer categories. Hence, I hope this package developed here is able to fill up the gap.&lt;/p&gt;

&lt;p&gt;Some examples that you may use this tool are,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Employment rate&lt;/li&gt;
&lt;li&gt;Demographics rate&lt;/li&gt;
&lt;li&gt;Basketball field goal % decomposed into 2 point vs 3 point&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To start using this package, you may first install the devtools package and execute the following command. install_github(&amp;ldquo;jironghuang/shiftshare&amp;rdquo;).&lt;/p&gt;

&lt;p&gt;And if you are interested in the codes used to develop the package, you may visit the following link &lt;a href=&#34;https://github.com/jironghuang/shiftshare&#34; target=&#34;_blank&#34;&gt;https://github.com/jironghuang/shiftshare&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You may follow the example to better understand how this package works. Essentially, what the example does is to decomposte the 29.7% point change into 3 effects. Within effect == 68.8%,  Across_effect == -7.4% and Dynamic Effect == -31.8%&lt;/p&gt;

&lt;h3 id=&#34;quick-example&#34;&gt;Quick example&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;emp1 = c(10, 20, 40, 50)
pop1 = c(40, 50, 60, 70)
emp2 = c(20, 30, 50, 60)
pop2 = c(50, 70, 20, 50)

ss_analysis &amp;lt;- shift_share(ap_grp_labels = c(&amp;quot;grp1&amp;quot;, &amp;quot;grp2&amp;quot;, &amp;quot;grp3&amp;quot;, &amp;quot;grp4&amp;quot;),
                           ap_numerator1 = emp1,
                           ap_numerator2 = emp2,
                           ap_denominator1 = pop1,
                           ap_denominator2 = pop2)
                           
&amp;gt; ss_analysis$get_effects()
  grp_labels     prop1     prop2     rate1     rate2 within_effect across_effect dynamic_effect overall_effect
1       grp1 0.1818182 0.2631579 0.2500000 0.4000000   0.027272727    0.02033493    0.012200957     0.05980861
2       grp2 0.2272727 0.3684211 0.4000000 0.4285714   0.006493506    0.05645933    0.004032809     0.06698565
3       grp3 0.2727273 0.1052632 0.6666667 2.5000000   0.500000000   -0.11164274   -0.307017544     0.08133971
4       grp4 0.3181818 0.2631579 0.7142857 1.2000000   0.154545455   -0.03930280   -0.026725906     0.08851675  

&amp;gt; ss_analysis$get_agg_effects()
         Description within_effect across_effect dynamic_effect overall_effect
1 aggregated_effects     0.6883117   -0.07415129     -0.3175097      0.2966507

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Automated Email Notification of my ETF watchlist</title>
      <link>/post/automated_email_notification/</link>
      <pubDate>Tue, 04 Sep 2018 01:37:53 +0800</pubDate>
      
      <guid>/post/automated_email_notification/</guid>
      <description>&lt;p&gt;I wrote an automated email notification code to send out my daily ETF watchlist in csv - an extension of my ETF watchlist project &lt;a href=&#34;http://jirong-huang.netlify.com/project/watch_list/&#34;&gt;here&lt;/a&gt;. I figured out that people will not visit my site. So why not blast out the watchlist instead:)&lt;/p&gt;

&lt;p&gt;And if you are interested in the code. Here you go.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Steps for sending watchlist
library(&amp;quot;rJava&amp;quot;)
library(&#39;mailR&#39;)

source(&amp;quot;./R/emails.R&amp;quot;)

# Write the content of your email
msg &amp;lt;- paste(&amp;quot;Hey there, I&#39;m sending this ETF watchlist that is updated as of &amp;quot;, 
             &amp;quot;\n&amp;quot;,
             as.character(date()),
             &amp;quot;\n&amp;quot;,
             &amp;quot;This is part of my daily automated ETF dashboard + Email notification and I thought you may be interested in it. See the following link for more details: http://jirong-huang.netlify.com/project/watch_list/&amp;quot;,
             &amp;quot;\n&amp;quot;,
             &amp;quot;If this irritates you too much, let me know and I can take you out of this mailing list:)&amp;quot;,&amp;quot;&amp;quot;,&amp;quot;Best,&amp;quot;,&amp;quot;Jirong&amp;quot;, sep = &amp;quot;\n&amp;quot;)

# Define who the sender is
sender &amp;lt;- &amp;quot;jironghuang88@gmail.com&amp;quot;
# Define who should get your email
recipients &amp;lt;- emails
              # Send your email with the send.mail function
              send.mail(from = sender,
                        to = recipients,
                        subject = &amp;quot;ETF Watchlist&amp;quot;,
                        body = msg,
                        attach.files = &amp;quot;./Output/yahoo_crawled_data.csv&amp;quot;,
                        smtp = list(host.name = &amp;quot;smtp.gmail.com&amp;quot;, port = 587,
                                    user.name = &amp;quot;jironghuang88@gmail.com&amp;quot;,
                                    passwd = Sys.getenv(&amp;quot;mail&amp;quot;), ssl = TRUE),
                        authenticate = TRUE,
                        send = TRUE)
              
              # JAVA_HOME /usr/lib/jvm/java-8-oracle             
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Asset Alllocation</title>
      <link>/asset_allocation/asset_allocation/</link>
      <pubDate>Sun, 02 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/asset_allocation/asset_allocation/</guid>
      <description>&lt;p&gt;This section indicates my overall current asset allocation, not based on any quantitative analysis but based on rule-of-thumb from wealth management community.&lt;/p&gt;

&lt;iframe src=&#34;https://docs.google.com/spreadsheets/d/e/2PACX-1vQtSJfzakpUWRkryIoXaqJm7szd-g6R1SHr-aAXAlHNOFEDXYGhCBNC9UeYEYv8cYf8krgsS6LPpED9/pubchart?oid=963907902&amp;amp;format=interactive&#34; width=&#34;1000&#34; height=&#34;1000&#34; frameborder=&#34;0&#34; marginheight=&#34;0&#34; marginwidth=&#34;0&#34;&gt;Loading...&lt;/iframe&gt;
</description>
    </item>
    
  </channel>
</rss>
